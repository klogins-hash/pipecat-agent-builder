"""Code generation templates for Pipecat agents."""

from typing import Dict, Any, List
from jinja2 import Environment, BaseLoader, Template
from core.config import AgentRequirements, AIServiceConfig
from core.logger import setup_logger

logger = setup_logger("templates")


class PipecatTemplateGenerator:
    """Generate Pipecat agent code from templates."""
    
    def __init__(self):
        self.env = Environment(loader=BaseLoader())
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict[str, Template]:
        """Load all code templates."""
        return {
            "main_bot": self.env.from_string(MAIN_BOT_TEMPLATE),
            "dockerfile": self.env.from_string(DOCKERFILE_TEMPLATE),
            "requirements": self.env.from_string(REQUIREMENTS_TEMPLATE),
            "deploy_config": self.env.from_string(DEPLOY_CONFIG_TEMPLATE),
            "knowledge_processor": self.env.from_string(KNOWLEDGE_PROCESSOR_TEMPLATE),
            "env_example": self.env.from_string(ENV_TEMPLATE),
        }
    
    def generate_agent_files(self, requirements: AgentRequirements) -> Dict[str, str]:
        """Generate all files for a Pipecat agent."""
        
        # Prepare template context
        context = self._prepare_context(requirements)
        
        # Generate all files
        files = {}
        for template_name, template in self.templates.items():
            try:
                content = template.render(**context)
                filename = self._get_filename(template_name, requirements)
                files[filename] = content
                logger.debug(f"Generated {filename}")
            except Exception as e:
                logger.error(f"Error generating {template_name}: {e}")
        
        return files
    
    def _prepare_context(self, requirements: AgentRequirements) -> Dict[str, Any]:
        """Prepare template rendering context."""
        
        # Service configurations
        stt_config = requirements.stt_service or AIServiceConfig(
            name="deepgram", provider="deepgram", model="nova-2"
        )
        llm_config = requirements.llm_service or AIServiceConfig(
            name="openai", provider="openai", model="gpt-4o"
        )
        tts_config = requirements.tts_service or AIServiceConfig(
            name="cartesia", provider="cartesia", voice_id="71a7ad14-091c-4e8e-a314-022ece01c121"
        )
        
        # Transport selection based on channels
        transport_type = "daily" if "phone" in requirements.channels else "webrtc"
        
        return {
            "agent_name": requirements.name,
            "agent_description": requirements.description,
            "personality": requirements.personality,
            "use_case": requirements.use_case,
            "channels": requirements.channels,
            "languages": requirements.languages,
            "stt_service": stt_config,
            "llm_service": llm_config,
            "tts_service": tts_config,
            "transport_type": transport_type,
            "knowledge_sources": requirements.knowledge_sources,
            "integrations": requirements.integrations,
            "deployment": requirements.deployment,
            "has_knowledge": len(requirements.knowledge_sources) > 0,
            "is_multilingual": len(requirements.languages) > 1,
            "supports_phone": "phone" in requirements.channels,
            "supports_web": "web" in requirements.channels,
        }
    
    def _get_filename(self, template_name: str, requirements: AgentRequirements) -> str:
        """Get appropriate filename for template."""
        filename_map = {
            "main_bot": "bot.py",
            "dockerfile": "Dockerfile",
            "requirements": "requirements.txt",
            "deploy_config": "pcc-deploy.toml",
            "knowledge_processor": "knowledge_processor.py",
            "env_example": ".env.example",
        }
        return filename_map.get(template_name, f"{template_name}.txt")


# Template definitions
MAIN_BOT_TEMPLATE = """#!/usr/bin/env python3
\"\"\"
{{ agent_name }} - {{ agent_description }}

Generated by Pipecat Agent Builder
Use case: {{ use_case }}
Channels: {{ channels | join(", ") }}
Languages: {{ languages | join(", ") }}
\"\"\"

import asyncio
import os
import sys
from typing import AsyncGenerator

from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineTask, PipelineParams
from pipecat.pipeline.runner import PipelineRunner
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.frames.frames import LLMRunFrame
{% if transport_type == "daily" %}
from pipecat.transports.services.daily import DailyParams, DailyTransport
{% else %}
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport, SmallWebRTCTransportParams
{% endif %}

# Service imports
{% if stt_service.provider == "deepgram" %}
from pipecat.services.deepgram.stt import DeepgramSTTService
{% elif stt_service.provider == "openai" %}
from pipecat.services.openai.stt import OpenAISTTService
{% endif %}

{% if llm_service.provider == "openai" %}
from pipecat.services.openai.llm import OpenAILLMService
{% elif llm_service.provider == "anthropic" %}
from pipecat.services.anthropic.llm import AnthropicLLMService
{% endif %}

{% if tts_service.provider == "cartesia" %}
from pipecat.services.cartesia.tts import CartesiaTTSService
{% elif tts_service.provider == "elevenlabs" %}
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
{% elif tts_service.provider == "openai" %}
from pipecat.services.openai.tts import OpenAITTSService
{% endif %}

{% if has_knowledge %}
from knowledge_processor import KnowledgeProcessor
{% endif %}

from pipecat.runner.run import create_transport, RunnerArguments
from pipecat.runner.run import main as runner_main

# Logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def create_services():
    \"\"\"Create and configure AI services.\"\"\"
    
    # Speech-to-Text
    {% if stt_service.provider == "deepgram" %}
    stt = DeepgramSTTService(
        api_key=os.getenv("DEEPGRAM_API_KEY"),
        model="{{ stt_service.model or 'nova-2' }}",
        language="{{ languages[0] if languages else 'en' }}"
    )
    {% elif stt_service.provider == "openai" %}
    stt = OpenAISTTService(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="{{ stt_service.model or 'whisper-1' }}"
    )
    {% endif %}
    
    # Large Language Model
    {% if llm_service.provider == "openai" %}
    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="{{ llm_service.model or 'gpt-4o' }}"
    )
    {% elif llm_service.provider == "anthropic" %}
    llm = AnthropicLLMService(
        api_key=os.getenv("ANTHROPIC_API_KEY"),
        model="{{ llm_service.model or 'claude-3-sonnet-20240229' }}"
    )
    {% endif %}
    
    # Text-to-Speech
    {% if tts_service.provider == "cartesia" %}
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="{{ tts_service.voice_id or '71a7ad14-091c-4e8e-a314-022ece01c121' }}"
    )
    {% elif tts_service.provider == "elevenlabs" %}
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"),
        voice_id="{{ tts_service.voice_id or 'pNInz6obpgDQGcFmaJgB' }}"
    )
    {% elif tts_service.provider == "openai" %}
    tts = OpenAITTSService(
        api_key=os.getenv("OPENAI_API_KEY"),
        voice="{{ tts_service.voice_id or 'alloy' }}"
    )
    {% endif %}
    
    return stt, llm, tts


async def create_context():
    \"\"\"Create conversation context with system message.\"\"\"
    
    system_message = f\"\"\"You are {{ agent_name }}, {{ agent_description }}.
    
    Your personality: {{ personality }}
    
    {% if has_knowledge %}
    You have access to a knowledge base that you can search to provide accurate information.
    {% endif %}
    
    {% if is_multilingual %}
    You can communicate in the following languages: {{ languages | join(", ") }}.
    Respond in the same language the user speaks to you in.
    {% endif %}
    
    {% if supports_phone %}
    You are currently speaking with someone over the phone. Keep responses conversational and natural.
    {% endif %}
    
    Be helpful, accurate, and engaging in your responses.\"\"\"
    
    messages = [{"role": "system", "content": system_message}]
    return OpenAILLMContext(messages)


async def run_bot(transport, runner_args: RunnerArguments):
    \"\"\"Main bot logic.\"\"\"
    
    # Create services
    stt, llm, tts = await create_services()
    
    # Create context and aggregator
    context = await create_context()
    context_aggregator = llm.create_context_aggregator(context)
    
    {% if has_knowledge %}
    # Initialize knowledge processor
    knowledge_processor = KnowledgeProcessor()
    await knowledge_processor.initialize()
    {% endif %}
    
    # Create pipeline
    pipeline = Pipeline([
        transport.input(),
        stt,
        context_aggregator.user(),
        {% if has_knowledge %}
        knowledge_processor,
        {% endif %}
        llm,
        tts,
        transport.output(),
        context_aggregator.assistant(),
    ])
    
    # Create task
    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )
    
    # Event handlers
    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected: {client}")
        # Add greeting
        context.add_message({
            "role": "system", 
            "content": "Greet the user and briefly introduce yourself."
        })
        await task.queue_frames([LLMRunFrame()])
    
    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected: {client}")
        await task.cancel()
    
    # Run pipeline
    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    \"\"\"Bot entry point.\"\"\"
    
    # Transport configuration
    transport_params = {
        {% if transport_type == "daily" %}
        "daily": lambda: DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
        {% else %}
        "webrtc": lambda: SmallWebRTCTransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
        {% endif %}
    }
    
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    runner_main()
"""

DOCKERFILE_TEMPLATE = """# {{ agent_name }} Dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    gcc \\
    g++ \\
    make \\
    libffi-dev \\
    libssl-dev \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Pre-download Silero VAD model
RUN python -c "import torch; torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)"

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Run the bot
CMD ["python", "bot.py"]
"""

REQUIREMENTS_TEMPLATE = """# {{ agent_name }} Requirements

# Core Pipecat
pipecat-ai[all]>=0.0.50

# Service-specific dependencies
{% if stt_service.provider == "deepgram" %}
pipecat-ai[deepgram]
{% elif stt_service.provider == "openai" %}
pipecat-ai[openai]
{% endif %}

{% if llm_service.provider == "openai" %}
pipecat-ai[openai]
{% elif llm_service.provider == "anthropic" %}
pipecat-ai[anthropic]
{% endif %}

{% if tts_service.provider == "cartesia" %}
pipecat-ai[cartesia]
{% elif tts_service.provider == "elevenlabs" %}
pipecat-ai[elevenlabs]
{% elif tts_service.provider == "openai" %}
pipecat-ai[openai]
{% endif %}

{% if transport_type == "daily" %}
pipecat-ai[daily]
{% else %}
pipecat-ai[webrtc]
{% endif %}

{% if has_knowledge %}
# Knowledge processing
chromadb>=0.4.0
sentence-transformers>=2.2.0
beautifulsoup4>=4.12.0
requests>=2.31.0
{% endif %}

# Utilities
python-dotenv>=1.0.0
asyncio>=3.4.3
"""

DEPLOY_CONFIG_TEMPLATE = """# {{ agent_name }} Deployment Configuration

agent_name = "{{ agent_name.lower().replace(' ', '-') }}"
image = "YOUR_DOCKERHUB_USERNAME/{{ agent_name.lower().replace(' ', '-') }}:latest"
secret_set = "{{ agent_name.lower().replace(' ', '-') }}-secrets"

[scaling]
min_agents = {{ deployment.scaling_min }}
max_agents = {{ deployment.scaling_max }}

[environment]
region = "{{ deployment.region }}"
environment = "{{ deployment.environment }}"

{% if supports_phone %}
[telephony]
enabled = true
{% endif %}
"""

KNOWLEDGE_PROCESSOR_TEMPLATE = """\"\"\"Knowledge processing for {{ agent_name }}.\"\"\"

import os
import asyncio
from typing import List, Dict, Any, Optional
import aiohttp
from bs4 import BeautifulSoup
import chromadb
from sentence_transformers import SentenceTransformer

from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.frames.frames import Frame, TextFrame, LLMRunFrame


class KnowledgeProcessor(FrameProcessor):
    \"\"\"Process knowledge queries and inject context.\"\"\"
    
    def __init__(self):
        super().__init__()
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        
    async def initialize(self):
        \"\"\"Initialize knowledge processing components.\"\"\"
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chroma_client = chromadb.PersistentClient(path="./knowledge_db")
        self.collection = self.chroma_client.get_or_create_collection("agent_knowledge")
        
        # Load knowledge sources
        await self._load_knowledge_sources()
    
    async def _load_knowledge_sources(self):
        \"\"\"Load and process knowledge sources.\"\"\"
        knowledge_sources = [
            {% for source in knowledge_sources %}
            {
                "type": "{{ source.type }}",
                "source": "{{ source.source }}",
                "options": {{ source.processing_options | tojson }}
            },
            {% endfor %}
        ]
        
        for source in knowledge_sources:
            if source["type"] == "web":
                await self._process_web_source(source["source"])
            elif source["type"] == "document":
                await self._process_document_source(source["source"])
    
    async def _process_web_source(self, url: str):
        \"\"\"Process web-based knowledge source.\"\"\"
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    content = await response.text()
                    soup = BeautifulSoup(content, 'html.parser')
                    text = soup.get_text()
                    
                    # Store in vector database
                    await self._store_knowledge(text, {"source": url, "type": "web"})
        except Exception as e:
            print(f"Error processing web source {url}: {e}")
    
    async def _process_document_source(self, file_path: str):
        \"\"\"Process document-based knowledge source.\"\"\"
        # Implementation for document processing
        pass
    
    async def _store_knowledge(self, text: str, metadata: Dict[str, Any]):
        \"\"\"Store knowledge in vector database.\"\"\"
        # Chunk text
        chunks = self._chunk_text(text)
        
        # Generate embeddings
        embeddings = self.embedding_model.encode(chunks)
        
        # Store in ChromaDB
        ids = [f"chunk_{i}" for i in range(len(chunks))]
        metadatas = [metadata for _ in chunks]
        
        self.collection.upsert(
            documents=chunks,
            embeddings=embeddings.tolist(),
            metadatas=metadatas,
            ids=ids
        )
    
    def _chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        \"\"\"Split text into chunks.\"\"\"
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
        return chunks
    
    async def search_knowledge(self, query: str, n_results: int = 3) -> List[str]:
        \"\"\"Search knowledge base for relevant information.\"\"\"
        if not self.collection:
            return []
        
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        return results["documents"][0] if results["documents"] else []
    
    async def process_frame(self, frame: Frame, direction: FrameDirection) -> AsyncGenerator[Frame, None]:
        \"\"\"Process frames and inject knowledge context.\"\"\"
        yield frame
        
        # If this is a user text frame, search for relevant knowledge
        if isinstance(frame, TextFrame) and direction == FrameDirection.DOWNSTREAM:
            relevant_info = await self.search_knowledge(frame.text)
            
            if relevant_info:
                # Create context injection
                context_text = "Relevant information:\\n" + "\\n".join(relevant_info)
                context_frame = TextFrame(context_text)
                yield context_frame
"""

ENV_TEMPLATE = """# {{ agent_name }} Environment Variables

# AI Service API Keys
{% if stt_service.provider == "deepgram" %}
DEEPGRAM_API_KEY=your_deepgram_api_key
{% endif %}
{% if stt_service.provider == "openai" or llm_service.provider == "openai" or tts_service.provider == "openai" %}
OPENAI_API_KEY=your_openai_api_key
{% endif %}
{% if llm_service.provider == "anthropic" %}
ANTHROPIC_API_KEY=your_anthropic_api_key
{% endif %}
{% if tts_service.provider == "cartesia" %}
CARTESIA_API_KEY=your_cartesia_api_key
{% endif %}
{% if tts_service.provider == "elevenlabs" %}
ELEVENLABS_API_KEY=your_elevenlabs_api_key
{% endif %}

{% if transport_type == "daily" %}
# Daily.co (for WebRTC)
DAILY_API_KEY=your_daily_api_key
{% endif %}

{% if supports_phone %}
# Telephony (if using phone channels)
TWILIO_ACCOUNT_SID=your_twilio_account_sid
TWILIO_AUTH_TOKEN=your_twilio_auth_token
{% endif %}

# Pipecat Cloud (for deployment)
PIPECAT_CLOUD_API_KEY=your_pipecat_cloud_api_key
PIPECAT_CLOUD_ORG_ID=your_organization_id

# Application Settings
DEBUG=false
LOG_LEVEL=INFO
"""
